#!/usr/bin/env python

"""Scikit-learn based dimensionality reduction analysis of SNP data.

This tool implements PCA, tSNE, and (maybe others in the future).

The ipyrad-analysis PCA tool is especially designed for working with
missing data that is common to RAD-seq datasets. Several options are
available for filtering and imputation. If no imputation argument is
provided then missing data (after filtering) is filled by the
randomly sampling alleles at each site based on their frequency in
the entire sample. If imputation method is set to "sampled" then
alleles are randomly sampled from each population in the IMAP based
on frequencies.
"""

import os
import sys
from typing import Optional, Dict, List, Union
import toyplot
import numpy as np
import pandas as pd
from loguru import logger

try:
    from sklearn import decomposition
    from sklearn.cluster import KMeans
    from sklearn.manifold import TSNE
except ImportError:
    pass

from ipyrad.assemble.utils import IPyradError
from ipyrad.analysis.snps_extracter import SNPsExtracter
from ipyrad.analysis.snps_imputer import SNPsImputer
from ipyrad.analysis.vcf_to_hdf5 import VCFtoHDF5 as vcf_to_hdf5
from ipyrad.analysis.pca_drawing import Drawing

logger.bind(name='ipa')

_MISSING_SKLEARN = """
This ipyrad tool requires the library scikit-learn.
You can install it with the following command in a terminal.

>>> conda install scikit-learn -c conda-forge
"""

_IMPORT_VCF_INFO = """
Converting vcf to HDF5 using default ld_block_size: {}
Typical RADSeq data generated by ipyrad/stacks will ignore this value.
You can use the ld_block_size parameter of the PCA() constructor to change
this value.
"""


class PCA:
    """Principal components analysis of RAD-seq SNPs.

    The PCA analysis tool in ipyrad-analysis can be used to subsample
    SNPs based on genetic linkage (e.g., 1 SNP per locus or linkage
    block), as well as to reduce missing data (e.g., set mincov and
    imap/minmap arguments). Because no missing data is allowed in a
    PCA analysis, it enforces *imputation* of missing genotypes, which
    when combined with stringent filtering can be quite accurate.

    Imputation is performed using a population allele sampling method.
    Two alleles are sampled at missing SNP calls to get a new genotype
    as 0, 1, or 2, based on the frequency of derived alleles at that
    site in the population to which the sample belongs.

    Population assignments can be entered as an imap dictionary

    An optional K-means clustering approach is available to reduce
    the bias that may come from a-priori population assignments. TODO.

    Parameters
    ----------
    data: str (hdf5 or vcf file path)
        A general .vcf file or a .snps.hdf5 file produced by ipyrad.
    imap: dict or None
        Dictionary mapping population names to a list of sample names.
        The population assignments will be used for imputation.
    minmap: dict, int, float, or None
        Dict mapping population names to int or float values. If an
        int or float is entered then it is treated as a dict mapping
        all pop names in the imap dict to that value. If a site does
        not have data for (value) number of samples in each population,
        respectively, the site is filtered from the data set. If None
        then all populations are set to 1 (require at least one
        individual to have data from every population at every site.)
    mincov: float or integer
        If a site does not have data across this proportion of total
        samples in the data then it is filtered from the data set.
    minmaf: float
        The minimum minor allele frequency for a SNP to be retained
        in the dataset.
    kmeans_clusters: int
        The number of imap populations to cluster samples into using
        the kmeans clustering method. If None then kmeans clustering is
        not performed. The estimated imap dict from this operation
        will override any user input imap dict. However, the imap arg
        can still be used to subsample which samples will be included
        in this analysis. Only the minimum minmap
        value is used (since imap pops are estimated you cannot set
        diff values for each pop here), so you should set minmap as
        just an int or float value when using this option.
    kmeans_niters: int
        Number of iterations of kmeans clustering with decreasing
        mincov thresholds used to refine population clustering, and
        therefore to refine the imap groupings. Iterations use mincovs
        equally spaced between `kmeans_maxcov` and `mincov` to
        filter the dataset for which SNPs to include.
    kmeans_mincov_max: float
        The most stringent mincov used as the first iteration in
        kmeans clustering. This value must be > mincov arg.
    ld_block_size: int
        Only used during conversion of data imported as vcf.
        The size of linkage blocks (in base pairs) to split the vcf
        data into.

    Examples
    --------
    >>> tool = ipa.pca(data="/tmp/data.snps.hdf5", mincov=0.5)
    >>> tool.run(nreplicates=20, random_seed=123)
    >>> tool.draw();

    >>> imap = ipa.popfile_to_imap("/tmp/data.popfile.tsv")
    >>> tool = ipa.pca(data="/tmp/data.snps.hdf5", mincov=0.5, imap=imap)
    >>> tool.run(nreplicates=20, random_seed=123)
    >>> tool.draw();
    """
    def __init__(
        self,
        data,
        imap: Optional[Dict[str,List[str]]]=None,
        minmap: Union[int, Dict[str,float]]=None,
        mincov: Union[int,float]=0.1,
        minmaf: float=0.0,
        random_seed: Optional[int]=None,
        kmeans_clusters: Optional[int]=None,
        kmeans_mincov_max: Optional[float]=0.8,
        kmeans_niters: Optional[int]=5,
        ld_block_size: int=0,
        cores: int=1,
        ):

        # only check import at init
        if not sys.modules.get("sklearn"):
            raise IPyradError(_MISSING_SKLEARN)

        # init attributes
        self.data = os.path.realpath(os.path.expanduser(data))
        """Path to the snps HDF5 file."""
        self.mincov = mincov
        """The minimum required sample coverage across all samples."""
        self.minmaf = minmaf
        """The minimum frequency of the minor allele else SNP is filtered."""
        self.imap = imap
        """Dict mapping population names to a list of samples names."""
        self.minmap = minmap
        """The minimum required sample coverage in each imap population."""
        self._kmeans_mincov_max = kmeans_mincov_max
        """The highest mincov level to use in iterative kmeans imputation method."""
        self._kmeans_clusters = kmeans_clusters
        """The target number of imap populations to cluster samples into."""
        self._kmeans_niters = kmeans_niters
        """The number of iterations to perform iterative kmeans imputation."""
        self._random_seed = random_seed
        """Seed of random generator for 'sample' imputation method."""
        self._ld_block_size = ld_block_size
        """SNPs farther apart then the block size in bp are treated as unlinked."""
        self._cores = cores
        """Number of cores used to parallelize SNP filtering."""

        # attributes to be filled.
        self._loadings: np.ndarray=None
        """Dict of loadings for each replicate run on the PC axes."""
        self._variances: np.ndarray=None
        """Array of variances explained by each PC axis."""
        self.names: List[str]=None
        """List of names in the subsampled dataset (imap or full)."""
        self.genos: np.ndarray=None
        """Array of (nsamples, nsnps) as dtype=uint8 that passed filtering."""
        self.snpsmap: np.ndarray=None
        """Array of (nsnps, 2) with SNP linkage information."""
        self.missing_cells: int=0
        """The number of missing cells in the SNP matrix before imputation."""
        self.missing_percent: float=0
        """The percentage of missing cells int he SNP matrix before imputation."""
        self.missing_per_sample: pd.Series=None
        """A pandas Series with percentage missing per sample."""
        self.stats: pd.Series=None
        """A pandas Series with stats from SNP extraction & filtering."""

        # hidden attributes used internally
        self._model: str="PCA"
        """Stores the name of dimensionality reduction method last performed."""
        self._drawn_imap: Dict=None
        """Stores the imap used in the last drawing."""
        self._drawn_pstyles: List=None
        """Stores the marker styles used in the last drawing."""
        self._ext: 'ipa.snps_extracter'=None
        """The snps_extracter module used to filter data."""

        # initialize functions: parse data and impute.
        self._check_map_args()
        self._check_for_vcf_conversion()
        if not self._kmeans_clusters:
            self._load_snps_hdf5()
            self._impute_data()
        else:
            self._iterative_kmeans()
        self._calculate_missing()

    def _check_map_args(self):
        """Expand minmap to a dict and check."""
        if isinstance(self.minmap, (int, float)):
            self.minmap = {i: self.minmap for i in self.imap}
        elif isinstance(self.minmap, dict):
            assert self.minmap.keys() == self.imap.keys(), (
                "imap and minmap must have same keys.")
        elif self.minmap is None:
            self.minmap = {i: 1 for i in self.imap}
        else:
            raise IPyradError("minmap arg not recognized.")

    def _check_for_vcf_conversion(self):
        """Writes a new HDF5 file from VCF and sets self.data to HDF5.

        ld_block_size has no effect on RAD data (RAD loci are already
        delimited) and instead only works to split *other* types of
        input VCF data, such as from shotgun re-sequencing.
        """
        if self.data.endswith((".vcf", ".vcf.gz")):
            # requires a block size int
            if not self._ld_block_size:
                self._ld_block_size = 20000
                logger.warning(_IMPORT_VCF_INFO.format(self._ld_block_size))

            # run the converter and update self.data to new file path.
            converter = vcf_to_hdf5(
                name=self.data.split("/")[-1].split(".vcf")[0],
                data=self.data,
                ld_block_size=self._ld_block_size,
            )
            converter.run()
            self.data = converter.database

    def _load_snps_hdf5(self):
        """Load .snps and .snpsmap arrays from HDF5 and calc missingness.

        This is the initial loading of the data. The data in .genos
        will be updated by imputation. Also, the data extraction will
        be repeated and overwritten in the kmeans method if performed.
        """
        self._ext = SNPsExtracter(
            data=self.data,
            imap=self.imap,
            minmap=self.minmap,
            mincov=self.mincov,
            minmaf=self.minmaf,
        )

        # run snp extracter to load SNP data to .genos, .names, .snpsmap
        self._ext.run(log_level="INFO", cores=self._cores)
        self.names = self._ext.names
        self.genos = self._ext.genos
        self.stats = self._ext.stats

        # raise an error if not data passed filtering
        if not self.genos.size:
            raise IPyradError("No data passed filtering.")

    def _calculate_missing(self):
        """record missing data per sample."""
        self.missing_per_sample = pd.Series(
            index=self.names, name="missing", data=0
        )
        for name in self.names:
            nmissing = sum(self.genos[self.names.index(name)] == 9)
            self.missing_per_sample.loc[name] = (
                round(nmissing / self.genos.shape[1], 3))

    def _impute_data(self):
        """Impute self.genos in-place by filling missing (9) values."""
        SNPsImputer(
            data=self.genos,
            names=self.names,
            imap=self.imap,
            impute_method="sample",
            inplace=True,
            random_seed=self._random_seed,
        ).run()

    def _iterative_kmeans(self):
        """Perform iterative PCA-Kmeans clustering to filter and imputate.

        This iteratively performs imputation first allowing for very
        little missing data, and then later allowing for more missing
        data. Through the iterative steps it aims to estimate the best
        imap grouping of samples. This method thus reduces the bias
        of the user defining the imap a priori in the 'sample' method.

        # TODO:, actually keeps K=int the whole time
        This runs imputation first using a global IMAP (all same pop.)
        and allowing the `mincov` level of missing data (e.g., 0.5).
        A PCA is then performed on this data and samples projected
        into PC space. A K-means clustering is performed to assign
        samples to groups based on their distance in PC space. Next,
        this process is repeated using an IMAP estimated by kmeans
        clustering of samples into 2 groups. This is repeated until
        kmeans_niters steps have been performed (the number of imap
        populations in the final iter), and using the highest micov
        threshold (`kmeans_topcov`, e.g., 0.9) in the final iteration.
        """
        # require minmap to not be a dict.
        minmap = min(self.minmap.values())

        # the ML models to fit
        pca_model = decomposition.PCA()
        kmeans_model = KMeans(n_clusters=self._kmeans_clusters)

        # get the subsample of names to include in this analysis
        self.names = SNPsExtracter(self.data, imap=self.imap).names

        # start kmeans with a global imap
        kmeans_imap = {'global': self.names}

        # get global mincov as a float.
        gmincov = (
            self.mincov if isinstance(self.mincov, float)
            else self.mincov / len(self.names)
        )

        # iterate over mincovs evenly spaced over iterations
        iters = np.linspace(self._kmeans_mincov_max, gmincov, self._kmeans_niters)
        for idx, kmeans_mincov in enumerate(iters):
            logger.info(
                f"Kmeans clustering: iter={idx}, "
                f"K={self._kmeans_clusters}, "
                f"mincov={kmeans_mincov:.2f}, "
                f"minmap={minmap},"
            )

            # 1. Load orig data and filter with imap, minmap, mincov=step
            self._ext = SNPsExtracter(
                data=self.data,
                imap=kmeans_imap,
                minmap={i: minmap for i in kmeans_imap},
                mincov=kmeans_mincov,
                minmaf=self.minmaf,
            )
            log = ("INFO" if idx in (0, len(iters) - 1) else "DEBUG")
            self._ext.run(log_level=log, cores=self._cores)
            self.genos = self._ext.genos
            self.names = self._ext.names
            if not self.genos.size:
                raise IPyradError(
                    "No data passed filtering, try different settings, such "
                    "as a lower `minmap` or `kmeans_mincov_max`. To view "
                    "filtering details in each kmeans iteration set "
                    "ipa.set_loglevel('DEBUG') at the top of your notebook."
                )

            # 2. Impute missing data using current kmeans clusters
            SNPsImputer(
                data=self._ext.genos,
                names=self.names,
                imap=kmeans_imap,
                impute_method="sample",
                inplace=True,
                random_seed=self._random_seed,
            ).run()

            # x. On final iteration return this imputed array as the result
            if idx == len(iters) - 1:
                logger.info("imap dict inferred by iterative PCA and KMeans.")
                continue

            # 3. subsample unlinked SNPs
            subdata = self._ext.subsample_genos(log_level="INFO")

            # 4. PCA on new imputed data values
            pcadata = pca_model.fit_transform(subdata)

            # 5. Kmeans clustering to find new imap grouping
            kmeans_model.fit(pcadata)
            labels = np.unique(kmeans_model.labels_)
            kmeans_imap = {}
            for lab in labels:
                group = np.where(kmeans_model.labels_ == lab)[0]
                kmeans_imap[lab] = [self.names[j] for j in group]
            logger.debug(f"imap in iter {idx}: {kmeans_imap}")

    def loadings(self, rep=0):
        """Return a dataframe with the PC loadings for a specific rep."""
        try:
            data = pd.DataFrame(self._loadings[rep], index=self.names)
        except ValueError as err:
            raise IPyradError(
                "You must call run() before accessing the pcs.") from err
        return data

    def variances(self, rep=0):
        """Return a dataframe with the PC loadings for a specific rep."""
        try:
            data = pd.DataFrame(self._variances[rep], index=self.names)
        except ValueError as err:
            raise IPyradError(
                "You must call run() before accessing the pcs.") from err
        return data

    def _run(self, subsample: bool, random_seed: int, log_level: str):
        """A single iteration of possibly many performed by .run."""
        if subsample:
            data = self._ext.subsample_genos(random_seed, log_level)
        else:
            data = self.genos

        # decompose pca call
        model = decomposition.PCA()
        model.fit(data)
        loadings = model.transform(data)
        variance = model.explained_variance_ratio_
        self._model = "PCA"

        # return tuple with new coordinates and variance explained
        return loadings, variance

    def run(
        self,
        nreplicates: int=1,
        random_seed: Optional[int]=None,
        subsample: bool=True,
        ):
        """
        Decompose genotype array (.snps) into n_components axes.

        Parameters
        ----------
        nreplicates: int
            Number of replicate subsampled analyses to run. This is useful
            for exploring variation over replicate samples of unlinked SNPs.
            The .draw() function will show variation over replicates runs.
        random_seed: int
            Random number seed used if/when subsampling SNPs.
        subsample: bool
            Subsample one SNP per RAD locus to reduce effect of linkage.
        log_level: str
            DEBUG is most verbose, less for INFO, and then WARNING.

        Returns
        -------
        None
            The loadings on each axis can be viewed with the .loadings()
            function, and .variances attribute.
        """
        # default to 1 rep
        nreplicates = max(nreplicates, 1)

        # get random generator.
        rng = np.random.default_rng(random_seed)

        # get data points for all replicate runs
        datas = {}
        vexps = {}
        for idx in range(nreplicates):
            datas[idx], vexps[idx] = self._run(
                subsample=subsample,
                random_seed=rng.integers(2**31),
                log_level="DEBUG" if idx else "INFO",
            )

        # store results to object
        self._loadings = datas
        self._variances = vexps

    def draw(
        self,
        ax0: int=0,
        ax1: int=1,
        width: int=300,
        height: int=300,
        cycle: int=8,
        colors: List[str]=None,
        opacity: float=None,
        shapes: str=None,
        size: int=10,
        legend: bool=True,
        legend_width: int=100,
        label: str=None,
        outfile: str=None,
        imap: Optional[Dict[str,List[str]]]=None,
        axes: Optional['toyplot.coordinates.Cartesian']=None,
        centroids_only: bool=False,
        ):
        """Draw a scatterplot for data along two PC axes.

        This function will draw a plot for data generated from the
        last call to `.run()`. Options are used to style the drawing.
        If an IMAP dict was used for imputation then samples will be
        grouped by color into the IMAP groups. An optional imap can
        be provided here to override the one used previously, which
        only affects the color mapping, not the already imputed data.

        Parameters
        ----------
        ...TODO.
        """
        drawing = Drawing(
            tool=self,
            ax0=ax0,
            ax1=ax1,
            cycle=cycle,
            colors=colors,
            opacity=opacity,
            shapes=shapes,
            size=size,
            legend=legend,
            legend_width=legend_width,
            label=label,
            outfile=outfile,
            imap=imap,
            width=width,
            height=height,
            axes=axes,
            centroids_only=centroids_only,
        )
        canvas, axes, marks = drawing.run()

        # store information of last drawing for optional added legend.
        self._drawn_imap = drawing.imap
        self._drawn_pstyles = drawing.pstyles
        return canvas, axes, marks

    def draw_legend(self, axes, **kwargs):
        """Draw legend on a cartesian axes.

        This is intended to be added to a custom setup canvas and
        axes configuration in toyplot. It is often simpler to just
        use the legend=True argument in the .draw() function.

        Example
        -------
        >>> import toyplot
        >>> canvas = toyplot.Canvas(width=1000, height=300)
        >>> ax0 = canvas.cartesian(bounds=(50, 250, 50, 250))
        >>> ax1 = canvas.cartesian(bounds=(350, 550, 50, 250))
        >>> ax2 = canvas.cartesian(bounds=(650, 850, 50, 250))
        >>> ax3 = canvas.cartesian(bounds=(875, 950, 50, 250))

        >>> pca.draw(0, 1, axes=ax0, legend=False)
        >>> pca.draw(0, 2, axes=ax1, legend=False)
        >>> pca.draw(1, 3, axes=ax2, legend=False);
        >>> pca.draw_legend(ax3, **{"font-size": "14px"})
        """
        # bail out if no drawing exists to add legend to.
        if not hasattr(self, "_drawn_pstyles"):
            logger.warning("You must first call .draw() to store a drawing.")
            return

        style = {
            "fill": "#262626",
            "text-anchor": "start",
            "-toyplot-anchor-shift": "15px",
            "font-size": "14px",
        }
        style.update(kwargs)
        skeys = sorted(self._drawn_imap, reverse=True)
        axes.scatterplot(
            np.repeat(0, len(self._drawn_imap)),
            np.arange(len(self._drawn_imap)),
            marker=[self._drawn_pstyles[i] for i in skeys],
        )
        axes.text(
            np.repeat(0, len(self._drawn_imap)),
            np.arange(len(self._drawn_imap)),
            list(skeys), #[i for i in skeys],
            style=style,
        )
        axes.show = False

    def draw_panels(self, pc0=0, pc1=1, pc2=2, **kwargs):
        """A convenience function for drawing a three-part panel plot.

        Three 2-way comparisons are shown among three selected axes.

        Parameters
        -----------
        pc0: int
        pc1: int
        pc2: int
        **kwargs: dict of style arguments to .draw() function.

        Example
        -------
        >>> pca.draw_panels();

        Or, the same can be accomplished with:
        >>> import toyplot
        >>> canvas = toyplot.Canvas(width=1000, height=300)
        >>> ax0 = canvas.cartesian(bounds=(50, 250, 50, 250))
        >>> ax1 = canvas.cartesian(bounds=(350, 550, 50, 250))
        >>> ax2 = canvas.cartesian(bounds=(650, 850, 50, 250))
        >>> ax3 = canvas.cartesian(bounds=(875, 950, 50, 250))
        >>> pca.draw(0, 1, axes=ax0, legend=False)
        >>> pca.draw(0, 2, axes=ax1, legend=False)
        >>> pca.draw(1, 3, axes=ax2, legend=False);
        >>> pca.draw_legend(ax3, **{"font-size": "14px"})
        """
        if self._model != "PCA":
            logger.warning("You must first call .run() to infer PC axes.")
            return None

        canvas = toyplot.Canvas(width=1000, height=300)
        ax0 = canvas.cartesian(bounds=(50, 250, 50, 250))
        ax1 = canvas.cartesian(bounds=(350, 550, 50, 250))
        ax2 = canvas.cartesian(bounds=(650, 850, 50, 250))
        ax3 = canvas.cartesian(bounds=(870, 950, 50, 250))

        self.draw(pc0, pc1, axes=ax0, legend=False, **kwargs)
        self.draw(pc0, pc2, axes=ax1, legend=False, **kwargs)
        self.draw(pc1, pc2, axes=ax2, legend=False, **kwargs)
        self.draw_legend(ax3, **{"font-size": "14px"})
        return canvas

    def run_and_draw(
        self,
        ax0: int=0,
        ax1: int=1,
        nreplicates: int=1,
        subsample: bool=True,
        random_seed: Optional[int]=None,
        ):
        """Performs PCA analysis and returns a 2D plot of results.

        Calls .run() and .draw() in one single call. This is simply
        a shortcut for to two. In general you will probably want to
        call them separately since there are more options.
        """
        self.run(
            nreplicates=nreplicates,
            subsample=subsample,
            random_seed=random_seed,
        )
        return self.draw(ax0=ax0, ax1=ax1)

    def run_umap(
        self,
        subsample: bool=True,
        random_seed: Optional[int]=None,
        n_neighbors: int=15,
        **kwargs,
        ):
        """
        Run UMAP dimensionality reduction method.

        Parameters
        ----------
        subsample: bool
            Subsamples SNPs to keep only 1 random SNP per linkage block.
        random_seed: int
            Seed of random number generator.
        n_neighbors: int
            Key parameter of UMAP clustering. Try several values.
        **kwargs: dict
            Other parameters supported by UMAP function. See UMAP docs.
        """
        # check just-in-time install
        try:
            import umap
        except ImportError as err:
            raise ImportError(
                "to use this function you must first install umap with:\n"
                "  >>> conda install umap-learn -c conda-forge "
                ) from err

        # subsample SNPS
        if subsample:
            data = self._ext.subsample_genos(random_seed)
        else:
            data = self.genos

        # init TSNE model object with params (sensitive)
        umap_kwargs = {
            'n_neighbors': int(n_neighbors),
            'init': 'spectral',
            'random_state': random_seed,
        }
        umap_kwargs.update(kwargs)
        umap_model = umap.UMAP(**umap_kwargs)

        # fit the model
        umap_data = umap_model.fit_transform(data)
        self._loadings = {0: umap_data}
        self._variances = {0: np.array([-1.0, -2.0])}
        self._model = "UMAP"

    def run_tsne(
        self,
        subsample: bool=True,
        perplexity: float=5.0,
        n_iters=1e6,
        random_seed=None,
        **kwargs,
        ):
        """Runs t-SNE model from scikit-learn on SNP data.

        Perplexity is the primary parameter affecting the TSNE, but
        any additional params supported by scikit-learn can be
        supplied as kwargs (see sklearn docs).
        """
        if subsample:
            data = self._ext.subsample_genos(random_seed)
        else:
            data = self.genos

        # init TSNE model object with params (sensitive)
        tsne_kwargs = {
            'perplexity': perplexity,
            'init': 'pca',
            'n_iter': int(n_iters),
            'random_state': random_seed,
        }
        tsne_kwargs.update(kwargs)
        tsne_model = TSNE(**tsne_kwargs)

        # fit the model
        tsne_data = tsne_model.fit_transform(data)
        self._loadings = {0: tsne_data}
        self._variances = {0: np.array([-1.0, -2.0])}
        self._model = "TSNE"
