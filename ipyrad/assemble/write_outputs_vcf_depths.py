#!/usr/bin/env python

"""Fetch SNP site depth information for a single sample.

This uses information in the tmpdir/chunk-x-y.npy files which
contain shift-table information generated by ChunkProcessor for
DENOVO data containing info about the position of variable sites
in the consens calls after accounting for trimmed edges and 
alignment changes that have occurred after step5.

Input
-----
>>> lidx  sidx  cidx  pos  shift
>>>    0     0     0    0      0
>>>    0     1    22    0     -2
>>>    0     2    12    0     -2

Output for each sample
----------------------
>>> lidx   pos   depth   C   A   T   G
>>>    0    14      10   0   0   0  10
>>>    0    22      10   0   0   0  10
>>>    0    83      10   0  10   0   0
>>>    1     3      20   0   0  20   0
>>>   ...   # skips any loci and sites where this sample is missing.
>>>    3     x      

Eventual combined file (is this needed? Or just load each sample ...)
----------------------
>>> lidx   pos  sidx    depth   s1          s2          s3           s4  ...
>>>    0    14     0      350   9:0,0,0,9   0:0,0,0,0   10:0,0,0,10  ...

"""

from typing import List, Iterator, Tuple, Dict
from pathlib import Path
import numpy as np
import h5py

class SingleDepths:
    def __init__(self, data: 'Assembly', sname: str):
        self.data = data
        self.sample = self.data.samples[sname]
        self.sidx = sorted(self.data.samples).index(sname)
        self.outfile = self.data.tmpdir / f"{sname}.depths.csv"
        self.outfile.unlink(missing_ok=True)
        print(sname)

    def _get_sorted_npys(self) -> List[Path]:
        """Gather all loci bits and sort by filename. And get snppad len."""
        return sorted(
            Path(self.data.tmpdir).glob("chunk-*.npy"),
            key=lambda x: int(x.name.split('-')[1]))

    def _get_catgs(self) -> np.ndarray:
        """..."""
        with h5py.File(self.sample.files.depths, 'r') as io5:
            return io5["catgs"][:]

    def _iter_loci(self) -> Iterator[Tuple[int, np.ndarray]]:
        """Yield each locus one at a time from full loaded array.
    
        The shift table arrays are chunked to be each pretty small
        so we can load each entire one in at a time without concern.

        >>> 0  0    0   4  0
        >>> 0  1   33   4  0
        >>> 0  2   22   4  1
        >>> 0  3  500   4  1
        >>> 0  0   32  10  0
        >>> 0  1  302  10  0   # yields locus 0
        >>> ...
        >>> 1  0  ...
        >>> ...                # yields locus 1
        """
        for npy in self._get_sorted_npys():
            arr = np.load(npy)
            lidxs = np.unique(arr[:, 0])
            for lidx in lidxs:
                yield lidx, arr[arr[:, 0] == lidx]

    def _iter_snvs(self) -> Iterator[Tuple[int, int, np.ndarray]]:
        """Yield a single variable site for samples w/ data.
        
        Note: this discard the raw locus ID (lidx) and replaces it
        with an enumeration of locus IDs ordered from 0-nloci in chunk.

        >>> 0  0    0   4  0
        >>> 0  1   33   4  0
        >>> 0  2   22   4  1
        >>> 0  3  500   4  1   # yields locus 0 site 4
        >>> 0  0   32  10  0
        >>> 0  1  302  10  0   # yields locus 0 site 10
        """
        # for lidx, loc in enumerate(self._iter_loci()):
        for lidx, loc in self._iter_loci():
            for pos in np.unique(loc[:, 3]):
                yield lidx, pos, loc[loc[:, 3] == pos]

    def _iter_sample_depths(self) -> Iterator[Tuple[int,int,int,int,int,int,int]]:
        """Yields a list with pos and depth info at all SNPs.

        >>> lidx   pos depth   C   A   T   G   # no actual header
        >>>   0     20    33  33   0   0   0
        >>>   0     88    33   0  33   0   0
        >>>   0    120    33  33   0   0   0
        >>> ...
        >>> 980     15    12   0   0  12   0        
        """
        depths = self._get_catgs()
        for lidx, pos, sloc in self._iter_snvs():
            # select only this sample at this locus for all sites
            srow = sloc[sloc[:, 1] == self.sidx]
            if not srow.size:
                # yield lidx, pos, 0, 0, 0, 0, 0
                yield 0, 0, 0, 0, 0                
            else:
                _, _, cidx, _, shift = srow[0]
                # if this was an indel (999 special val) then return empty
                if shift == 999:
                    # yield lidx, pos, 0, 0, 0, 0, 0
                    yield 0, 0, 0, 0, 0                    
                # else return the depths from the catg database.
                else:
                    catg = depths[cidx][pos + shift]                    
                    # try:
                        # catg = depths[cidx][pos + shift]
                    # except IndexError:
                        # err = [lidx, pos, cidx, shift, sum(catg)] + list(catg)
                        # raise SystemExit(f"errror in {self.sample.name}: err: {err}")
                    row = [sum(catg)] + list(catg)                
                    yield row

    def run(self) -> np.ndarray:
        """Fetches all SNP depth info re-aligned and into an array."""
        return np.array(list(self._iter_sample_depths()), dtype=int)

    def write(self) -> None:
        """Write depths data to a tmpfile."""
        np.savetxt(self.outfile, self.run(), delimiter=",", fmt="%d")


class CombinedDepths:
    """
    
    The main purpose of this class is to create a generator to read 
    from all of the individual sample TSV depth files and yield a 
    list of data with information for the INFO and FORMAT columns
    of the VCF file.

    Writes each sample tmp depths file for every SNP site. Then joins
    these all together into a 

    Combined file format
    --------------------
    >>> lidx   pos   depth   s1          s2          s3           s4  ...
    >>>    0    14     350   9:0,0,0,9   0:0,0,0,0   10:0,0,0,10  ...

    """
    def __init__(self, data: 'Assembly', samples: Dict[str, "SampleSchema"]):
        self.data = data
        self.samples = samples

        self.snames: List[str] = None
        """: Ordered list of sample names."""
        self.open_files: List['fd'] = []
        """: Ordered list of open depth files for each sample."""
        self._get_snames()

    def _get_snames(self) -> None:
        """Get sorted names with 'reference' on top if present."""
        self.snames = sorted(self.samples)
        if self.data.drop_ref:
            self.snames.remove("reference")
        elif self.data.is_ref:
            self.snames.remove("reference")
            self.snames = ['reference'] + self.snames

    def open_handles(self):
        """Fill self.open_files with open file handles."""
        for sname in self.snames:
            if sname != "reference":
                path = self.data.tmpdir / f"{sname}.depths.csv"
                ofd = open(path, 'r', encoding="utf-8")
                self.open_files.append(ofd)

    def iter_snp_depths(self) -> Iterator[List[str]]:
        """Generator from each sample's ordered depth file in order."""        
        # create composite generator to yield one line from each
        generators = zip(*[iter(i) for i in self.open_files])
        for line in generators:
            yield [i.strip().split(",", 1) for i in line]

    def close_handles(self):
        """Close each fd in self.open_files"""
        for ofd in self.open_files:
            print(f"closing {ofd}")
            ofd.close()
            # ofd.unlink()


if __name__ == "__main__":

    import ipyrad as ip

    DATA = ip.load_json("../../tests/ipsim.json")
    DATA = ip.load_json("../../sra-fastqs/cyatho.json")

    DATA.tmpdir = DATA.params.project_dir / f"{DATA.name}_tmp_outfiles"
    DATA.drop_ref = False

    for _sname in DATA.samples:
        NEW = SingleDepths(DATA, _sname)
        NEW.write()

    C = CombinedDepths(DATA, DATA.samples)
    C.open_handles()
    i = C.iter_snp_depths()
    # for j in i:
        # lidx = (j[0][0])
        # print(lidx, j[2])
    C.close_handles()
    # for _ in range(5):
        # print(next(i))
    # print(NEW.outfile)
    # ARR = NEW.run()
    # print(ARR)

    # 1C_0
    # 85,25,48,48,0,0,0
    # 85,34,48,0,0,0,48
    # 85,41,48,0,0,48,0    --- should be zeros...
    # 85,64,48,48,0,0,0

    # 2G_0  
    # 85,25,34,34,0,0,0
    # 85,34,34,0,34,0,0
    # 85,41,34,0,0,34,0
    # 85,64,34,34,0,0,0


    # DATA = ip.load_json("../../sra-fastqs/cyatho.json")
    # DATA.tmpdir = Path("../../sra-fastqs/cyatho_tmp_outfiles/")
    # SAMPLE = DATA.samples["29154_superba_
    # get_depths()


    # t = Spacer(DATA, DATA.samples)
    # t.run()
    # i = t._iter_loci()
    # print(next(i))


